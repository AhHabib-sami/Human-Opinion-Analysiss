# -*- coding: utf-8 -*-
"""Copy of nn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ArVcBmYgs61MDjwCW3GPnGYbfj4CID7A
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Thesis/Reviews(1).csv'

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Extract the 'Text' and 'Score' columns
text_data = df['Text'].values
score_data = df['Score'].values

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(text_data, score_data, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Print the shapes of the training, validation, and test data
print(f"Training Data Shape: {X_train.shape}")
print(f"Validation Data Shape: {X_val.shape}")
print(f"Test Data Shape: {X_test.shape}")

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# Set the maximum sequence length based on your requirements
max_sequence_length = 100

def data_generator(text_data, score_data, tokenizer, batch_size):
    num_samples = len(text_data)
    steps_per_epoch = int(np.ceil(num_samples / batch_size))

    while True:
        for i in range(steps_per_epoch):
            start_idx = i * batch_size
            end_idx = (i + 1) * batch_size

            batch_text = text_data[start_idx:end_idx]
            batch_score = score_data[start_idx:end_idx]

            sequences = tokenizer.texts_to_sequences(batch_text)
            padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)

            yield padded_sequences, batch_score

# Example usage of the generator
batch_size = 128
train_generator = data_generator(X_train, y_train, tokenizer, batch_size)
val_generator = data_generator(X_val, y_val, tokenizer, batch_size)

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='linear')  # Linear activation for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Print the model summary
model.summary()

# Print the shapes of the training, validation, and test data again after tokenization
print(f"Training Data Shape after Tokenization: {X_train.shape}")
print(f"Validation Data Shape after Tokenization: {X_val.shape}")
print(f"Test Data Shape after Tokenization: {X_test.shape}")

# Train the model using the generator
history = model.fit(train_generator,
                    steps_per_epoch=len(X_train)//batch_size,
                    epochs=5,
                    validation_data=val_generator,
                    validation_steps=len(X_val)//batch_size)

import matplotlib.pyplot as plt

# Plot the training history with nice formatting
def plot_training_history(history):
    plt.figure(figsize=(12, 6))

    # Plot training & validation loss values
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot training & validation accuracy values
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE', color='blue')
    plt.plot(history.history['val_mae'], label='Validation MAE', color='orange')
    plt.title('Model Mean Absolute Error (MAE)')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot the training history
plot_training_history(history)

# Evaluate the model on the test set
# Tokenize the test text data
test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)

# Evaluate the model using the tokenized test data
test_loss, test_mae = model.evaluate(test_padded_sequences, y_test)
print(f'Test Mean Absolute Error: {test_mae*100}')

# Replace 'your_text' with the text you want to test
input_text = "I must be a bit of a wuss, because this soup tastes to me how I imagine fire might taste. Typically I like spicy food if it has a good flavor.  I don't find this to be the case with this soup. Any flavor is killed off by the burn."

# Tokenize and pad the input text
input_sequence = tokenizer.texts_to_sequences([input_text])
input_padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length)

# Predict the score for the input text
predicted_score = model.predict(input_padded_sequence)

print(f'Predicted Score: {predicted_score[0][0]}')


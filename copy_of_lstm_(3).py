# -*- coding: utf-8 -*-
"""Copy of lstm (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19r7zoJfHq9aKUZREJMM2b4c0K2ZFw_XM
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/THESIS/sami/Reviews.csv'

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Extract the 'Text' and 'Score' columns
text_data = df['Text'].values
score_data = df['Score'].values

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(text_data, score_data, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Print the shapes of the training, validation, and test data
print(f"Training Data Shape: {X_train.shape}")
print(f"Validation Data Shape: {X_val.shape}")
print(f"Test Data Shape: {X_test.shape}")

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# Set the maximum sequence length based on your requirements
max_sequence_length = 100

def data_generator(text_data, score_data, tokenizer, batch_size):
    num_samples = len(text_data)
    steps_per_epoch = int(np.ceil(num_samples / batch_size))

    while True:
        for i in range(steps_per_epoch):
            start_idx = i * batch_size
            end_idx = (i + 1) * batch_size

            batch_text = text_data[start_idx:end_idx]
            batch_score = score_data[start_idx:end_idx]

            sequences = tokenizer.texts_to_sequences(batch_text)
            padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)

            yield padded_sequences, batch_score

# Example usage of the generator
batch_size = 128
train_generator = data_generator(X_train, y_train, tokenizer, batch_size)
val_generator = data_generator(X_val, y_val, tokenizer, batch_size)

# Build the model with an LSTM layer
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='linear')  # Linear activation for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Print the model summary
model.summary()

# Print the shapes of the training, validation, and test data again after tokenization
print(f"Training Data Shape after Tokenization: {X_train.shape}")
print(f"Validation Data Shape after Tokenization: {X_val.shape}")
print(f"Test Data Shape after Tokenization: {X_test.shape}")

# Train the model using the generator
history = model.fit(train_generator,
                    steps_per_epoch=len(X_train)//batch_size,
                    epochs=5,
                    validation_data=val_generator,
                    validation_steps=len(X_val)//batch_size)

import matplotlib.pyplot as plt

# Plot the training history with nice formatting
def plot_training_history(history):
    plt.figure(figsize=(12, 6))


    # Plot training & validation loss values
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot training & validation accuracy values
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE', color='blue')
    plt.plot(history.history['val_mae'], label='Validation MAE', color='orange')
    plt.title('Model Mean Absolute Error (MAE)')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot the training history
plot_training_history(history)

# Evaluate the model on the test set
# Tokenize the test text data
test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)

# Evaluate the model using the tokenized test data
test_loss, test_mae = model.evaluate(test_padded_sequences, y_test)
print(f'Test Mean Absolute Error: {test_mae}')

# Replace 'your_text' with the text you want to test
input_text = "I have a 4 year old male cat who has chronic urinary tract infections.  I feed him this dry food in combination with wet food (with water added to it) and Uri Ease.  The combination seems to keeps his UTI's under control.  If I switch to another type of dry food, his UTI gets worse.  The other cats seem to enjoy this food as well.  It's expensive but it seems to be the best solution for me."

# Tokenize and pad the input text
input_sequence = tokenizer.texts_to_sequences([input_text])
input_padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length)

# Predict the score for the input text
predicted_score = model.predict(input_padded_sequence)

print(f'Predicted Score: {predicted_score[0][0]}')

